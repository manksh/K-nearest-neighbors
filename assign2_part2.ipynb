{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assign2_part2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/manksh/K-nearest-neighbors/blob/master/assign2_part2.ipynb)"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "8TtFxZ8LMJ8f"
      },
      "cell_type": "markdown",
      "source": [
        "Assignment 2 - Part 2 \n",
        "\n",
        "Author: Manksh Gupta (mg3835)"
      ]
    },
    {
      "metadata": {
        "id": "svc9wjQ6FoCJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ]
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "1M2ciAUtFYYS"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "tfe = tf.contrib.eager\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g6ZhWBzUFoCP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.rcParams[\"figure.figsize\"] = (10,7)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J8saeUOhFoCR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data"
      ]
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "skQBAMSgFgJF"
      },
      "cell_type": "code",
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "# Dataset will be cached locally after you run this code\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize pixel values to [0, 1]\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# These types are required for the operation we use to compute\n",
        "# loss. Omit, and you shall receive a cryptic error message.\n",
        "y_train = y_train.astype(np.int32)\n",
        "y_test = y_test.astype(np.int32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "gcrB99XDYu_v"
      },
      "cell_type": "code",
      "source": [
        "slices = np.random.random(10000)<0.8\n",
        "x_validate = x_test[~slices]\n",
        "y_validate = y_test[~slices]\n",
        "x_test = x_test[slices]\n",
        "y_test = y_test[slices]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "_phAjMlEFsA0"
      },
      "cell_type": "code",
      "source": [
        "buffer_size = 5000\n",
        "batch_size = 100\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(buffer_size)\n",
        "train_dataset = train_dataset.batch(batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IdFfPnM0FoCa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Models"
      ]
    },
    {
      "metadata": {
        "id": "uN-_y9MWFoCa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Deep Model"
      ]
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "acHRNledFwXL"
      },
      "cell_type": "code",
      "source": [
        "class Model_deep(tf.keras.Model):\n",
        "      def __init__(self,a, active):\n",
        "        self.a = a\n",
        "        self.active = active\n",
        "        super(Model_deep, self).__init__()\n",
        "        self.flatten = tf.keras.layers.Flatten()\n",
        "        self.dense_1 = tf.keras.layers.Dense(128, activation=active, kernel_initializer=a)\n",
        "        self.dropout_1 = tf.keras.layers.Dropout(0.2)\n",
        "        self.dense_2 = tf.keras.layers.Dense(64, activation=active, kernel_initializer=a)\n",
        "        self.dropout_2 = tf.keras.layers.Dropout(0.2)\n",
        "        self.dense_3 = tf.keras.layers.Dense(64, activation=active, kernel_initializer=a)\n",
        "        self.dropout_3 = tf.keras.layers.Dropout(0.2)\n",
        "        self.dense_4 = tf.keras.layers.Dense(10)\n",
        "    # FIX ME\n",
        "    # add some layers to your model\n",
        "\n",
        "      def call(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.dense_1(x)\n",
        "        x = self.dropout_1(x)\n",
        "        x = self.dense_2(x)\n",
        "        x = self.dropout_2(x)\n",
        "        x = self.dense_3(x)\n",
        "        x = self.dropout_3(x)\n",
        "        x = self.dense_4(x)\n",
        "        \n",
        "    # FIX ME\n",
        "    # use your layers (don't forget to add activation functions here as well\n",
        "    # if you haven't specified them in your layer definintions)\n",
        "    \n",
        "    \n",
        "        return x # be sure to return logits, not softmax output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4v00cDeiFoCd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_deep(model_deep, images, labels, optim):\n",
        "  # You should not need to modify this function\n",
        "  with tf.GradientTape() as tape:\n",
        "    logits = model_deep(images)\n",
        "    loss_value = loss(logits, labels)  \n",
        "    grads = tape.gradient(loss_value, model_deep.variables)\n",
        "    optim.apply_gradients(zip(grads, model_deep.variables))\n",
        "\n",
        "  return loss_value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BM3imtYXFoCe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Linear Model"
      ]
    },
    {
      "metadata": {
        "id": "7ZbvlcFmFoCf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Model_linear(tf.keras.Model):\n",
        "      def __init__(self, a):\n",
        "        self.a = a\n",
        "        super(Model_linear, self).__init__()\n",
        "        self.flatten = tf.keras.layers.Flatten()\n",
        "        self.dense_4 = tf.keras.layers.Dense(10, kernel_initializer=self.a)\n",
        "    # FIX ME\n",
        "    # add some layers to your model\n",
        "\n",
        "      def call(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.dense_4(x)\n",
        "        \n",
        "    # FIX ME\n",
        "    # use your layers (don't forget to add activation functions here as well\n",
        "    # if you haven't specified them in your layer definintions)\n",
        "    \n",
        "    \n",
        "        return x # be sure to return logits, not softmax output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7r_qsaiiFoCh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_linear(model_linear, images, labels, optim):\n",
        "  # You should not need to modify this function\n",
        "  with tf.GradientTape() as tape:\n",
        "    logits = model_linear(images)\n",
        "    loss_value = loss(logits, labels)  \n",
        "  grads = tape.gradient(loss_value, model_linear.variables)\n",
        "  optim.apply_gradients(zip(grads, model_linear.variables))\n",
        "  return loss_value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nNwQ6M8tFoCj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Common Functions"
      ]
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "oi0611F5GFyK"
      },
      "cell_type": "code",
      "source": [
        "def loss(logits, labels):\n",
        "  # FIX ME\n",
        "  # You will need to modify this function, of course.\n",
        "  # Best bet, use tf.nn.sparse_softmax_cross_entropy_with_logits\n",
        "  # though if you're interested, you can write your own.\n",
        "    \n",
        "  return tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits = logits, labels = labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "2vaut5jVGuwP"
      },
      "cell_type": "code",
      "source": [
        "def compute_accuracy(logits, labels):\n",
        "  # You shoud not need to modify this function\n",
        "  predictions = tf.argmax(logits, axis=1)\n",
        "  batch_size = int(logits.shape[0])\n",
        "  return tf.reduce_sum(tf.cast(tf.equal(predictions, labels), dtype=tf.float32)) / batch_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6qloT6baFoCn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# The first time you run the below block it will crash\n",
        "# with an error 'ValueError: No variables provided.''\n",
        "# This is because the call method of your model\n",
        "# is not using any trainable variables.\n",
        "# (As written, the model just flattens the images.)\n",
        "\n",
        "def run_model(mod, optimizer, epoch):\n",
        "    optimizer = optimizer\n",
        "    model = mod\n",
        "    epochs = epoch\n",
        "    step_counter = 0\n",
        "    train_accu = []\n",
        "    validation_accu = []\n",
        "    train_loss = []\n",
        "\n",
        "    for epoch_n in range(epochs):\n",
        "      for (batch, (images, labels)) in enumerate(train_dataset):\n",
        "        loss_value = train_linear(model, images, labels, optim = optimizer)\n",
        "        step_counter +=1\n",
        "        train_loss.append(loss_value)\n",
        "        \n",
        "        \n",
        "\n",
        "\n",
        "      train_accuracy = compute_accuracy(model(x_train), y_train)\n",
        "      validation_accuracy = compute_accuracy(model(x_validate), y_validate)\n",
        "\n",
        "\n",
        "      train_accu.append(train_accuracy)\n",
        "      validation_accu.append(validation_accuracy)\n",
        "        \n",
        "\n",
        "    test_accu = compute_accuracy(model(x_test), y_test)\n",
        "    \n",
        "    return train_accu, validation_accu, test_accu, train_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DAI2cpiyFoCp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Linear Model"
      ]
    },
    {
      "metadata": {
        "id": "F-124WebFoCp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here, I am training a linear model and visualizing the loss."
      ]
    },
    {
      "metadata": {
        "id": "9FhY9ioRFoCq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_accu, validation_accu, test_accu, train_loss = run_model(mod = Model_linear('glorot_uniform_initializer'), \n",
        "                                                   optimizer = tf.train.AdamOptimizer(learning_rate = 0.001), epoch = 15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "05q-YBm0FoCr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.plot(train_loss, label = 'loss')\n",
        "plt.axes().yaxis.grid(True)\n",
        "plt.axes().xaxis.grid(True)\n",
        "plt.title('Visualizing Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1BCntsLnFoCt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"# create a log directory\n",
        "logdir = os.mkdir(\"logs\")\n",
        "\n",
        "# run an experiment and write logs\n",
        "writer = tf.contrib.summary.create_file_writer(logdir=\"logs/exp_1\", flush_millis=1000)\n",
        "with writer.as_default():\n",
        "    with tf.contrib.summary.always_record_summaries():\n",
        "        for i,j in enumerate(list(np.array(train_loss))):\n",
        "            loss = j\n",
        "            tf.contrib.summary.scalar(\"loss\", loss, step=i)\n",
        "writer.close()\n",
        "\n",
        "!tensorboard --logdir=logs\n",
        "# open your browser http://localhost:6006/\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "3GyUAz5yFoCu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.plot(train_accu, label = 'train_accuracy')\n",
        "plt.plot(validation_accu, label = 'validation_accuracy ')\n",
        "plt.axes().yaxis.grid(True)\n",
        "plt.axes().xaxis.grid(True)\n",
        "plt.title('Visualizing Accuracy')\n",
        "plt.legend()\n",
        "plt.ylim(0.5,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TLnSFLsAFoCx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, I implement a linear model to make predictions and I will try different learning rates, different optimizers,  different weight initializations. Since this model has only 1 layer(output layer), I do not try different activations and stick to softmax."
      ]
    },
    {
      "metadata": {
        "id": "r_MUuWUeFoCx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Different Optimizers"
      ]
    },
    {
      "metadata": {
        "id": "dCgo6JQ_FoCy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "I will try the following optimizers:\n",
        "\n",
        "1) Adam Optimizer\n",
        "\n",
        "2) Vanilla Gradient Descent\n",
        "\n",
        "3) AdaGrad Optimizer"
      ]
    },
    {
      "metadata": {
        "id": "dUnvvqaLFoCz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_accu, validation_accu, test_accu, train_loss = run_model(mod = Model_linear('glorot_uniform_initializer'), \n",
        "                                                   optimizer = tf.train.AdamOptimizer(learning_rate = 0.001), epoch = 15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nMqfYgaYFoC0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_accu1, validation_accu1, test_accu1, train_loss1 = run_model(mod = Model_linear('glorot_uniform_initializer'), \n",
        "                                                   optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.001), epoch = 15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n-OwdlLAFoC2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_accu2, validation_accu2, test_accu2, train_loss2 = run_model(mod = Model_linear('glorot_uniform_initializer'), \n",
        "                                                   optimizer = tf.train.AdagradOptimizer(learning_rate = 0.001), epoch = 15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b_LbLHYPFoC4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.plot(train_loss, label = 'Adam')\n",
        "plt.plot(train_loss1, label = 'Vanilla Gradient Descent')\n",
        "plt.plot(train_loss2, label = 'AdaGrad')\n",
        "plt.axes().yaxis.grid(True)\n",
        "plt.axes().xaxis.grid(True)\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HmIMMpa2FoC6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.plot(train_accu, label = 'train_accuracy - Adam')\n",
        "plt.plot(validation_accu, label = 'validation_accuracy - Adam')\n",
        "plt.plot(train_accu1, label = 'train_accuracy - Vanilla Gradient Descent')\n",
        "plt.plot(validation_accu1, label = 'validation_accuracy - Vanilla Gradient Descent')\n",
        "plt.plot(train_accu2, label = 'train_accuracy - AdaGrad')\n",
        "plt.plot(validation_accu2, label = 'validation_accuracy - AdaGrad')\n",
        "plt.axes().yaxis.grid(True)\n",
        "plt.axes().xaxis.grid(True)\n",
        "plt.legend()\n",
        "plt.ylim(0.5,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yKaD_EiRFoC9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Different Learning Rates"
      ]
    },
    {
      "metadata": {
        "id": "28DpthIDFoC-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "I will now try different learning rates. In order to do this, I will fix Adam as the optimizer(as it worked the best) and try different learning rates.\n",
        "\n",
        "1) 1 ( High learning rate)\n",
        "\n",
        "2) 0.001 (Reasonable learning rate)\n",
        "\n",
        "3) 0.0000001 (Low learning rate)"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "5xyJWSHDFoC-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_accu, validation_accu, test_accu, train_loss = run_model(mod = Model_linear('glorot_uniform_initializer'), \n",
        "                                                   optimizer = tf.train.AdamOptimizer(learning_rate = 1), epoch = 15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "llRFccx3FoDA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_accu1, validation_accu1, test_accu1, train_loss1 = run_model(mod = Model_linear('glorot_uniform_initializer'), \n",
        "                                                   optimizer = tf.train.AdamOptimizer(learning_rate = 0.001), epoch = 15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g8VF2U9hFoDC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_accu2, validation_accu2, test_accu2, train_loss2 = run_model(mod = Model_linear('glorot_uniform_initializer'), \n",
        "                                                   optimizer = tf.train.AdamOptimizer(learning_rate = 0.0000001), epoch = 15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "moI9AG9dFoDD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.plot(train_loss, label = 'learning_rate = 1')\n",
        "plt.plot(train_loss1, label = 'learning_rate = 0.001')\n",
        "plt.plot(train_loss2, label = 'learning_rate = 0.0000001')\n",
        "plt.axes().yaxis.grid(True)\n",
        "plt.axes().xaxis.grid(True)\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K2HGgzu5FoDE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.plot(train_accu, label = 'train_accuracy - learning_rate = 1')\n",
        "plt.plot(validation_accu, label = 'validation_accuracy - learning_rate = 1')\n",
        "plt.plot(train_accu1, label = 'train_accuracy - learning_rate = 0.001')\n",
        "plt.plot(validation_accu1, label = 'validation_accuracy - learning_rate = 0.001')\n",
        "plt.plot(train_accu2, label = 'train_accuracy - learning_rate = 0.0000001')\n",
        "plt.plot(validation_accu2, label = 'validation_accuracy - learning_rate = 0.0000001')\n",
        "plt.axes().yaxis.grid(True)\n",
        "plt.axes().xaxis.grid(True)\n",
        "plt.legend()\n",
        "plt.ylim(0.5,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QjQV0rm5FoDF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For the linear model, significant difference is seen when changing the learning rate for the linear model. Learning rate of 0.001 seems to be the most optimal looking at the loss values."
      ]
    },
    {
      "metadata": {
        "id": "itWLDUWoFoDG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Different Weight Initializations"
      ]
    },
    {
      "metadata": {
        "id": "-riCeleDFoDG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "I will now try different weight initialization stratergys for the model. I will try:\n",
        "\n",
        "1) Default\n",
        "\n",
        "2) Random Uniform\n",
        "\n",
        "3) Random Normal"
      ]
    },
    {
      "metadata": {
        "id": "QO7lkofOFoDH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_accu, validation_accu, test_accu, train_loss = run_model(mod = Model_linear('glorot_uniform_initializer'), \n",
        "                                                   optimizer = tf.train.AdamOptimizer(learning_rate = 0.001), epoch = 15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PqRUiwEgFoDI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_accu1, validation_accu1, test_accu1, train_loss1 = run_model(mod = Model_linear('RandomUniform'), \n",
        "                                                   optimizer = tf.train.AdamOptimizer(learning_rate = 0.001), epoch = 15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HtO6gLKGFoDK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_accu2, validation_accu2, test_accu2, train_loss2 = run_model(mod = Model_linear('RandomNormal'), \n",
        "                                                   optimizer = tf.train.AdamOptimizer(learning_rate = 0.001), epoch = 15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JDHFsis9FoDL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.plot(train_loss, label = 'Initializer = Default')\n",
        "plt.plot(train_loss1, label = 'Initializer = RandomUniform')\n",
        "plt.plot(train_loss2, label = 'Initializer - RandomNormal')\n",
        "plt.axes().yaxis.grid(True)\n",
        "plt.axes().xaxis.grid(True)\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DlZa1oZYFoDM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.plot(train_accu, label = 'train_accuracy - Initializer = Default')\n",
        "plt.plot(validation_accu, label = 'validation_accuracy - Initializer = Default')\n",
        "plt.plot(train_accu1, label = 'train_accuracy - Initializer = RandomUniform')\n",
        "plt.plot(validation_accu1, label = 'validation_accuracy - Initializer = RandomUniform')\n",
        "plt.plot(train_accu2, label = 'train_accuracy - Initializer - RandomNormal')\n",
        "plt.plot(validation_accu2, label = 'validation_accuracy - Initializer - RandomNormal')\n",
        "plt.axes().yaxis.grid(True)\n",
        "plt.axes().xaxis.grid(True)\n",
        "plt.legend()\n",
        "plt.ylim(0.5,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "na8vZ_pLFoDN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We see that the best one is the default one, the other ones take longer to converge."
      ]
    },
    {
      "metadata": {
        "id": "0qW3lZMMFoDN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Deep Model"
      ]
    },
    {
      "metadata": {
        "id": "mZtt1i_OFoDO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here, I am training a Deep model and visualizing the loss."
      ]
    },
    {
      "metadata": {
        "id": "lH6Ab0lqFoDO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_accu, validation_accu, test_accu, train_loss = run_model(mod = Model_deep('glorot_uniform_initializer', active = tf.nn.relu), \n",
        "                                                   optimizer = tf.train.AdamOptimizer(learning_rate = 0.001), epoch = 15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rJpo75tJFoDP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.plot(train_loss, label = 'Loss')\n",
        "plt.axes().yaxis.grid(True)\n",
        "plt.axes().xaxis.grid(True)\n",
        "plt.title('Visualizing Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t3gYFdVdFoDQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"# create a log directory\n",
        "logdir = os.mkdir(\"logs\")\n",
        "\n",
        "# run an experiment and write logs\n",
        "writer = tf.contrib.summary.create_file_writer(logdir=\"logs/exp_1\", flush_millis=1000)\n",
        "with writer.as_default():\n",
        "    with tf.contrib.summary.always_record_summaries():\n",
        "        for i,j in enumerate(list(np.array(train_loss))):\n",
        "            loss = j\n",
        "            tf.contrib.summary.scalar(\"loss\", loss, step=i)\n",
        "writer.close()\n",
        "\n",
        "!tensorboard --logdir=logs\n",
        "# open your browser http://localhost:6006/\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IBRTdsU8FoDR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.plot(train_accu, label = 'train_accuracy')\n",
        "plt.plot(validation_accu, label = 'validation_accuracy ')\n",
        "plt.axes().yaxis.grid(True)\n",
        "plt.axes().xaxis.grid(True)\n",
        "plt.title('Visualizing Accuracy')\n",
        "plt.legend()\n",
        "plt.ylim(0.5,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ijuV24-dFoDS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, I implement the Deep model to make predictions and I will try different learning rates, different optimizers,  different weight initializations and different activations."
      ]
    },
    {
      "metadata": {
        "id": "52wlq0zcFoDT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Different Optimizers"
      ]
    },
    {
      "metadata": {
        "id": "jXYjzTtUFoDT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "I will try the following optimizers:\n",
        "\n",
        "1) Adam Optimizer\n",
        "\n",
        "2) AdaDelta\n",
        "\n",
        "2) Vanilla Dradient Descent"
      ]
    },
    {
      "metadata": {
        "id": "TMEso3PRFoDT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_accu, validation_accu, test_accu, train_loss = run_model(mod = Model_deep('glorot_uniform_initializer', active = tf.nn.relu), \n",
        "                                                   optimizer = tf.train.AdamOptimizer(learning_rate = 0.001), epoch = 15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CYJ_DCtuFoDU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_accu1, validation_accu1, test_accu1, train_loss1 = run_model(mod = Model_deep('glorot_uniform_initializer', active = tf.nn.relu), \n",
        "                                                   optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.001), epoch = 15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l8S7DpIoFoDV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_accu2, validation_accu2, test_accu2, train_loss2 = run_model(mod = Model_deep('glorot_uniform_initializer', active = tf.nn.relu), \n",
        "                                                   optimizer = tf.train.AdagradOptimizer(learning_rate = 0.001), epoch = 15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UcKj1Rx4FoDW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.plot(train_loss, label = 'Adam')\n",
        "plt.plot(train_loss1, label = 'Vanilla Gradient Descent')\n",
        "plt.plot(train_loss2, label = 'AdaGrad')\n",
        "plt.axes().yaxis.grid(True)\n",
        "plt.axes().xaxis.grid(True)\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3wYLxrU7FoDY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.plot(train_accu, label = 'train_accuracy - Adam')\n",
        "plt.plot(validation_accu, label = 'validation_accuracy - Adam')\n",
        "plt.plot(train_accu1, label = 'train_accuracy - Vanilla Gradient Descent')\n",
        "plt.plot(validation_accu1, label = 'validation_accuracy - Vanilla Gradient Descent')\n",
        "plt.plot(train_accu2, label = 'train_accuracy - AdaGrad')\n",
        "plt.plot(validation_accu2, label = 'validation_accuracy - AdaGrad')\n",
        "plt.axes().yaxis.grid(True)\n",
        "plt.axes().xaxis.grid(True)\n",
        "plt.legend()\n",
        "plt.ylim(0.5,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "reUvrsZpFoDa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Different learning rates"
      ]
    },
    {
      "metadata": {
        "id": "4rZ0Wp8DFoDa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "I will now try different learning rates. In order to do this, I will fix Adam as the optimizer(as it performed the best) and try different learning rates.\n",
        "\n",
        "1) 1 ( High learning rate)\n",
        "\n",
        "2) 0.001 (Reasonable learning rate)\n",
        "\n",
        "3) 0.0000001 (Low learning rate)"
      ]
    },
    {
      "metadata": {
        "id": "5zgtWMHOFoDa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "train_accu, validation_accu, test_accu, train_loss = run_model(mod = Model_deep('glorot_uniform_initializer', active = tf.nn.relu), \n",
        "                                                   optimizer = tf.train.AdamOptimizer(learning_rate = 1), epoch = 15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VsucBd89FoDc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_accu1,validation_accu1, test_accu1, train_loss1 = run_model(mod = Model_deep('glorot_uniform_initializer', active = tf.nn.relu), \n",
        "                                                   optimizer = tf.train.AdamOptimizer(learning_rate = 0.001), epoch = 15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y-PcmZi-FoDd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_accu2, validation_accu2, test_accu2, train_loss2 = run_model(mod = Model_deep('glorot_uniform_initializer', active = tf.nn.relu), \n",
        "                                                   optimizer = tf.train.AdamOptimizer(learning_rate = 0.0000001), epoch = 15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M_yAgER9FoDe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.plot(train_loss, label = 'learning_rate = 1')\n",
        "plt.plot(train_loss1, label = 'learning_rate = 0.001')\n",
        "plt.plot(train_loss2, label = 'learning_rate = 0.0000001')\n",
        "plt.axes().yaxis.grid(True)\n",
        "plt.axes().xaxis.grid(True)\n",
        "plt.ylim(0,5)\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BDNavt7uFoDf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.plot(train_accu, label = 'train_accuracy - learning_rate = 1')\n",
        "plt.plot(validation_accu, label = 'validation_accuracy - learning_rate = 1')\n",
        "plt.plot(train_accu1, label = 'train_accuracy - learning_rate = 0.001')\n",
        "plt.plot(validation_accu1, label = 'validation_accuracy - learning_rate = 0.001')\n",
        "plt.plot(train_accu2, label = 'train_accuracy - learning_rate = 0.0000001')\n",
        "plt.plot(validation_accu2, label = 'validation_accuracy - learning_rate = 0.0000001')\n",
        "plt.axes().yaxis.grid(True)\n",
        "plt.axes().xaxis.grid(True)\n",
        "plt.legend()\n",
        "plt.ylim(0,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WDqzDw9ZFoDg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Different Weight Initializations"
      ]
    },
    {
      "metadata": {
        "id": "NXZ5z71bFoDh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "I will now try different weight initialization stratergys for the model. I will try:\n",
        "\n",
        "1) Default\n",
        "\n",
        "2) Random Uniform\n",
        "\n",
        "3) Random Normal"
      ]
    },
    {
      "metadata": {
        "id": "uDxM4GcbFoDi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_accu, validation_accu, test_accu, train_loss = run_model(mod = Model_deep('glorot_uniform_initializer', active = tf.nn.relu), \n",
        "                                                   optimizer = tf.train.AdamOptimizer(learning_rate = 0.001), epoch = 15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V39JQyKpFoDj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_accu1,validation_accu1, test_accu1, train_loss1 = run_model(mod = Model_deep('RandomUniform', active = tf.nn.relu), \n",
        "                                                   optimizer = tf.train.AdamOptimizer(learning_rate = 0.001), epoch = 15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Tk2VwdQMFoDl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_accu2, validation_accu2, test_accu2, train_loss2 = run_model(mod = Model_deep('RandomNormal', active = tf.nn.relu), \n",
        "                                                   optimizer = tf.train.AdamOptimizer(learning_rate = 0.001), epoch = 15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JZHueuSNFoDm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.plot(train_loss, label = 'Default')\n",
        "plt.plot(train_loss1, label = 'RandomUniform')\n",
        "plt.plot(train_loss2, label = 'RandomNormal')\n",
        "plt.axes().yaxis.grid(True)\n",
        "plt.axes().xaxis.grid(True)\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eQ_8R-1MFoDn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.plot(train_accu, label = 'train_accuracy - Default')\n",
        "plt.plot(validation_accu, label = 'validation_accuracy - Default')\n",
        "plt.plot(train_accu1, label = 'train_accuracy - RandomUniform')\n",
        "plt.plot(validation_accu1, label = 'validation_accuracy - RandomUniform')\n",
        "plt.plot(train_accu2, label = 'train_accuracy - RandomNormal')\n",
        "plt.plot(validation_accu2, label = 'validation_accuracy - RandomNormal')\n",
        "plt.axes().yaxis.grid(True)\n",
        "plt.axes().xaxis.grid(True)\n",
        "plt.legend()\n",
        "plt.ylim(0.5,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Bp3GNkqqFoDo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Different Activations"
      ]
    },
    {
      "metadata": {
        "id": "_86XKWwgFoDp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "I will now try different weight initialization stratergys for the model. I will try:\n",
        "\n",
        "1) relu\n",
        "\n",
        "2) sigmoid\n",
        "\n",
        "3) tanh"
      ]
    },
    {
      "metadata": {
        "id": "BoyUeI-BFoDp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_accu, validation_accu, test_accu, train_loss = run_model(mod = Model_deep('glorot_uniform_initializer', active = tf.nn.relu), \n",
        "                                                   optimizer = tf.train.AdamOptimizer(learning_rate = 0.001), epoch = 15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OGPgRn2YFoDr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_accu1, validation_accu1, test_accu1, train_loss1 = run_model(mod = Model_deep('glorot_uniform_initializer', active = tf.nn.sigmoid), \n",
        "                                                   optimizer = tf.train.AdamOptimizer(learning_rate = 0.001), epoch = 15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "idKRbBQpFoDs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_accu2, validation_accu2, test_accu2, train_loss2 = run_model(mod = Model_deep('glorot_uniform_initializer', active = tf.nn.tanh), \n",
        "                                                   optimizer = tf.train.AdamOptimizer(learning_rate = 0.001), epoch = 15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tHnq7KFVFoDu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.plot(train_loss, label = 'relu')\n",
        "plt.plot(train_loss1, label = 'sigmoid')\n",
        "plt.plot(train_loss2, label = 'tanh')\n",
        "plt.axes().yaxis.grid(True)\n",
        "plt.axes().xaxis.grid(True)\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_oDkTrO_FoDv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.plot(train_accu, label = 'train_accuracy - relu')\n",
        "plt.plot(validation_accu, label = 'validation_accuracy - relu')\n",
        "plt.plot(train_accu1, label = 'train_accuracy - sigmoid')\n",
        "plt.plot(validation_accu1, label = 'validation_accuracy - sigmoid')\n",
        "plt.plot(train_accu2, label = 'train_accuracy - tanh')\n",
        "plt.plot(validation_accu2, label = 'validation_accuracy - tanh')\n",
        "plt.axes().yaxis.grid(True)\n",
        "plt.axes().xaxis.grid(True)\n",
        "plt.legend()\n",
        "plt.ylim(0.5,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4cuub7vmN60w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}